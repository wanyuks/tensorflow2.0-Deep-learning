{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.dataset的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 从numpy中构建\n",
    "dataset = tf.data.Dataset.from_tensor_slices(np.arange(10))\n",
    "print(dataset)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset.interleave(lambda v: tf.data.Dataset.from_tensor_slices(v),\n",
    "                             cycle_length=5,\n",
    "                             block_length=5)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((2,), ()), types: (tf.float64, tf.string)>\n",
      "[1. 2.] b'cat'\n",
      "[3. 4.] b'dog'\n",
      "[5. 6.] b'fox'\n"
     ]
    }
   ],
   "source": [
    "# 从元组中构建\n",
    "\n",
    "x = np.array([[1., 2.], [3., 4.], [5., 6.]])\n",
    "y = np.array([\"cat\", \"dog\", \"fox\"])\n",
    "dataset3 = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "print(dataset3)\n",
    "for item_x, item_y in dataset3:\n",
    "    print(item_x.numpy(), item_y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2.] b'cat'\n",
      "[3. 4.] b'dog'\n",
      "[5. 6.] b'fox'\n"
     ]
    }
   ],
   "source": [
    "# 从字典中构建\n",
    "dataset4 = tf.data.Dataset.from_tensor_slices({\"feature\":x, \"label\": y})\n",
    "for item in dataset4:\n",
    "    print(item[\"feature\"].numpy(), item[\"label\"].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从csv文件中读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (11610, 8)\n",
      "validation shape:  (3870, 8)\n",
      "test shape:  (5160, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing()\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(housing.data,\n",
    "                                                           housing.target,\n",
    "                                                           test_size=0.25,\n",
    "                                                           random_state=7)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_all,\n",
    "                                                  y_train_all,\n",
    "                                                  test_size=0.25,\n",
    "                                                  random_state=7)\n",
    "print(\"train shape: \", x_train.shape)\n",
    "print(\"validation shape: \", x_val.shape)\n",
    "print(\"test shape: \", x_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"generate csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "def save_to_csv(output_dir, data, name_prefix, header=None, n_parts=10):\n",
    "    path_format = os.path.join(output_dir, \"{}_{:02d}.csv\")\n",
    "    file_names = []\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(len(data)), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        file_names.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding='utf-8') as f:\n",
    "            if header:\n",
    "                f.write(header+\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join(repr(col) for col in data[row_idx]))\n",
    "                f.write(\"\\n\")\n",
    "    return file_names\n",
    "\n",
    "train_data = np.c_[x_train_scaled, y_train]\n",
    "val_data = np.c_[x_val_scaled, y_val]\n",
    "test_data = np.c_[x_test_scaled, y_test]\n",
    "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "train_files = save_to_csv(output_dir, train_data, \"train\", header_str, n_parts=20)\n",
    "val_files = save_to_csv(output_dir, val_data, \"val\", header_str, n_parts=10)\n",
    "test_files = save_to_csv(output_dir, test_data, \"test\", header_str, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.io.decode_csv的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow中读完系列文件的过程\n",
    "1. 将所有需要读取的文件的文件名写入dataset\n",
    "2. 对于文件名生成的dataset去读取文件内容形成文件内容的satasets\n",
    "3. 解析文件中的内容（tf.io.decoder_csv()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'generate csv\\\\train_17.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_11.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_03.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_01.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_08.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_14.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_12.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_13.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_16.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_09.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_04.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_02.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_18.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_15.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_10.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_00.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_05.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_19.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_07.csv', shape=(), dtype=string)\n",
      "tf.Tensor(b'generate csv\\\\train_06.csv', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# 针对读取文件名有一个专门的api\n",
    "# keras.data.Dataset.list_files\n",
    "filename_dataset = tf.data.Dataset.list_files(train_files)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-0.7628576678376784,0.8238167412601372,0.10041321355962941,-0.06324778224149417,-0.2639595301791804,-0.06119686943141775,0.5297853063545601,-0.1085175850218531,0.719'\n",
      "b'0.8095613521801492,0.34518428537112195,0.22347987936517727,-0.10669826574327357,-0.17047084206891044,0.008573079922849763,-0.841838842223145,0.739231082317489,2.349'\n",
      "b'-0.45521966900167576,-0.2929923224808983,-0.3987316967655819,0.048342385355644925,0.8612736888777626,-0.03406098870043033,-0.6873555763423803,0.5996018900498323,2.042'\n",
      "b'-0.5153188550167574,1.6215375010751625,-0.5674383914596058,-0.09192162409389779,-0.6101203483172071,-0.1362930022629114,1.0353669037825188,-1.335259774230543,2.113'\n",
      "b'0.24184624203240349,1.6215375010751625,0.009664811030936506,-0.05126299611752959,0.0030306872168157756,-0.12848071177796017,-0.7575752426518185,0.5547210782495127,5.00001'\n",
      "b'-0.8602543240469787,0.5845005133156296,-0.5401804329592943,-0.29451363578528666,-0.18984237203770513,-0.10174016540818816,-0.6920368874296753,0.5746681057163238,2.21'\n",
      "b'0.5112344948394413,1.3024491971491523,0.13587658679582906,-0.10351732524563802,-0.6084358674503553,-0.10230906308235456,-0.7528939315645234,0.5347740507827086,5.00001'\n",
      "b'-0.26910264752187235,0.7440446652786346,-0.32686955387040534,-0.13957064331225846,-0.3801887099919485,0.0013605451901388182,-0.8699267087469216,0.6594429724502586,1.777'\n",
      "b'-1.2395775315541393,1.86085372901967,-1.1683362207894175,0.022679365237900546,0.2161175168735572,-0.06696920715665644,0.9979164150841523,-1.4050743703643713,2.375'\n",
      "b'-0.9716176924992204,-0.691852702388411,-0.2596016484740199,-0.1397123594639106,-0.5393721519094352,-0.07717107637960378,1.9856730545035917,-0.6670343540924798,0.709'\n",
      "b'1.5581559667165958,-0.9311689303329186,0.4852241981387221,-0.22752369366224293,-0.14015018646557967,0.032685256468747204,-0.9261024417944714,0.8040589215846128,3.329'\n",
      "b'3.5210609831933297,-0.4525364744439034,0.7946855709899874,-0.20096979493338954,-0.4290386551306481,-0.04368524825750971,0.7872574161558378,-1.2405113937632062,5.00001'\n",
      "b'1.5251437377787345,-0.05367609453639072,0.7812516902583831,-0.19689529591433746,-0.4214584912298154,0.0027580329308190734,-0.6499050876440104,0.5248005370492994,3.482'\n",
      "b'-0.9064397020321601,0.5845005133156296,-0.6994545730611924,-0.10441338979964694,-0.10477608826169373,0.18504601792673156,-0.7482126204772251,0.7791251372511042,1.621'\n",
      "b'-0.11414620752348001,1.0631329692046447,-0.07279432778010213,0.02413199105865309,-0.010445159717997912,-0.042861839895142875,0.9932351039968573,-1.4449684252979866,3.284'\n"
     ]
    }
   ],
   "source": [
    "# interleave会遍历filename_dataset的每一个元素，进行操作，然后将操作的结果组合起来\n",
    "# tf.data.TextLineDataset会按行去去读文件中的文本内容\n",
    "\n",
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename).skip(1),# 加skip是跳过第一行，避免header被读取到dataset中\n",
    "                                    cycle_length=n_readers)\n",
    "for item in dataset.take(15):\n",
    "    print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=492, shape=(8,), dtype=float32, numpy=\n",
       " array([-0.7628577 ,  0.8238167 ,  0.10041321, -0.06324778, -0.26395953,\n",
       "        -0.06119687,  0.52978534, -0.10851759], dtype=float32)>,\n",
       " <tf.Tensor: id=493, shape=(1,), dtype=float32, numpy=array([0.719], dtype=float32)>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def parse_csv_line(line, n_fields=9):\n",
    "        defs = [tf.constant(np.nan)] * n_fields\n",
    "        parse_line = tf.io.decode_csv(line, record_defaults=defs)\n",
    "        x = tf.stack(parse_line[:-1])\n",
    "        y = tf.stack(parse_line[-1:])\n",
    "        return x, y\n",
    "    \n",
    "parse_csv_line(b'-0.7628576678376784,0.8238167412601372,0.10041321355962941,-0.06324778224149417,-0.2639595301791804,-0.06119686943141775,0.5297853063545601,-0.1085175850218531,0.719',\n",
    "              n_fields=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tf.Tensor(\n",
      "[[-0.5153189   1.6215374  -0.56743836 -0.09192163 -0.61012036 -0.13629301\n",
      "   1.0353669  -1.3352598 ]\n",
      " [ 1.1558194  -0.13344817  0.6085721   0.04220942  1.167007    0.04454635\n",
      "  -0.73885     0.9087808 ]\n",
      " [-0.7826967  -0.7716248  -0.0938434   0.14471786  0.18748134 -0.02488174\n",
      "   0.20677485  0.33031702]], shape=(3, 8), dtype=float32)\n",
      "y:  tf.Tensor(\n",
      "[[2.113]\n",
      " [2.148]\n",
      " [0.9  ]], shape=(3, 1), dtype=float32)\n",
      "x:  tf.Tensor(\n",
      "[[-1.6167846  -0.45253646 -0.02914891  0.0877159  -0.31786293  0.02373059\n",
      "   1.007279   -1.3452333 ]\n",
      " [ 1.1680932  -1.4895735   0.3828349  -0.23364641  4.9141345   0.00602934\n",
      "   1.08218    -1.3352598 ]\n",
      " [-1.2395775   1.8608537  -1.1683363   0.02267936  0.21611752 -0.06696921\n",
      "   0.9979164  -1.4050744 ]], shape=(3, 8), dtype=float32)\n",
      "y:  tf.Tensor(\n",
      "[[1.125]\n",
      " [2.724]\n",
      " [2.375]], shape=(3, 1), dtype=float32)\n",
      "x:  tf.Tensor(\n",
      "[[ 0.8274959  -0.3727644  -0.02113135 -0.2520319   1.9907181   0.11129898\n",
      "   0.810664   -1.1756835 ]\n",
      " [ 0.16571708  0.5845005  -0.03475306 -0.17650913 -0.59327555  0.04182775\n",
      "   0.91365284 -1.2205644 ]\n",
      " [ 0.10783987  1.8608537  -0.46100593 -0.10480534 -0.4719929  -0.16016383\n",
      "   0.9791912  -1.4150479 ]], shape=(3, 8), dtype=float32)\n",
      "y:  tf.Tensor(\n",
      "[[2.641]\n",
      " [1.87 ]\n",
      " [3.556]], shape=(3, 1), dtype=float32)\n",
      "x:  tf.Tensor(\n",
      "[[-0.6978913  -0.45253646 -0.1331875   0.06136209  1.6622443  -0.07082807\n",
      "  -0.9448277   1.1182246 ]\n",
      " [-0.9064397   0.5845005  -0.69945455 -0.10441339 -0.10477609  0.18504602\n",
      "  -0.74821264  0.77912515]\n",
      " [ 0.20703527  0.50472844  0.45857033  0.05359804 -0.38776886 -0.01088222\n",
      "   0.9089715  -1.4499552 ]], shape=(3, 8), dtype=float32)\n",
      "y:  tf.Tensor(\n",
      "[[1.092]\n",
      " [1.621]\n",
      " [2.417]], shape=(3, 1), dtype=float32)\n",
      "x:  tf.Tensor(\n",
      "[[ 0.34458622 -1.5693456   0.10149456 -0.05198546  1.009508   -0.02858241\n",
      "  -0.73885     1.212973  ]\n",
      " [-0.73164415 -0.21322025  0.15980008  0.02820103 -0.5637971  -0.04452026\n",
      "   2.210376   -0.41270974]\n",
      " [-0.42395326  0.74404466 -0.24696815 -0.19506454  0.38961905 -0.05303082\n",
      "   1.1289932  -1.4748889 ]], shape=(3, 8), dtype=float32)\n",
      "y:  tf.Tensor(\n",
      "[[1.358]\n",
      " [0.856]\n",
      " [3.508]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 将前面的整合一下，定义出一个完整的函数\n",
    "def csv_reader_dataset(filenames, n_readers=5, batch_size=32, n_parse_thread=5,\n",
    "                      shuffler_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "                                cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffler_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_thread)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_set = csv_reader_dataset(train_files, batch_size=3)\n",
    "for x_batch, y_batch in train_set.take(5):\n",
    "    print(\"x: \",x_batch)\n",
    "    print(\"y: \",y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_files)\n",
    "val_set = csv_reader_dataset(val_files)\n",
    "test_set = csv_reader_dataset(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 348 steps, validate for 120 steps\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 1s 3ms/step - loss: 0.6063 - val_loss: 0.3919\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3828 - val_loss: 0.3541\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3542 - val_loss: 0.3547\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3420 - val_loss: 0.3168\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3197 - val_loss: 0.3109\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3167 - val_loss: 0.2989\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3011 - val_loss: 0.3023\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.3152 - val_loss: 0.3075\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2770 - val_loss: 0.3119\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 1s 2ms/step - loss: 0.2872 - val_loss: 0.2869\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=[8]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(\"adam\",loss='mse')\n",
    "history = model.fit(train_set,\n",
    "                epochs=10,\n",
    "                steps_per_epoch = 11160 // 32,\n",
    "                validation_steps = 3870 // 32,\n",
    "                validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 966us/step - loss: 0.3256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32561101787578983"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=5160//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
