{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tfrecord的学习使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mlt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tfrecord的文件格式\n",
    "tfrecord中存储的是:   \n",
    "-> tf.train.Example   \n",
    "  -> tf.train.Features -> {\"key\": tf.train.feature}    \n",
    "    -> tf.train.feature -> tf.train.ByteList(存储文本)/tf.train.FloatList(存储浮点数)/tf.train.Int64List(存储整型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: \"deep learning\"\n",
      "value: \"machine learning\"\n",
      "\n",
      "value: 1.2000000476837158\n",
      "value: 9.399999618530273\n",
      "value: 42.70000076293945\n",
      "value: 16.799999237060547\n",
      "\n",
      "value: 16\n",
      "\n",
      "feature {\n",
      "  key: \"age\"\n",
      "  value {\n",
      "    int64_list {\n",
      "      value: 16\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  key: \"favorite_books\"\n",
      "  value {\n",
      "    bytes_list {\n",
      "      value: \"deep learning\"\n",
      "      value: \"machine learning\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  key: \"hour\"\n",
      "  value {\n",
      "    float_list {\n",
      "      value: 1.2000000476837158\n",
      "      value: 9.399999618530273\n",
      "      value: 42.70000076293945\n",
      "      value: 16.799999237060547\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "favorite_books = [name.encode(\"utf-8\") for name in \n",
    "                  [\"deep learning\", \"machine learning\"]]\n",
    "favorite_books_bytelist = tf.train.BytesList(value=favorite_books)\n",
    "print(favorite_books_bytelist)\n",
    "\n",
    "hours_floatlist = tf.train.FloatList(value=[1.2, 9.4, 42.7, 16.8])\n",
    "print(hours_floatlist)\n",
    "\n",
    "age_list = tf.train.Int64List(value=[16])\n",
    "print(age_list)\n",
    "\n",
    "features = tf.train.Features(feature = {\n",
    "    \"favorite_books\": tf.train.Feature(bytes_list=favorite_books_bytelist),\n",
    "    \"hour\": tf.train.Feature(float_list=hours_floatlist),\n",
    "    \"age\": tf.train.Feature(int64_list=age_list)\n",
    "})\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"age\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 16\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"favorite_books\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"deep learning\"\n",
      "        value: \"machine learning\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"hour\"\n",
      "    value {\n",
      "      float_list {\n",
      "        value: 1.2000000476837158\n",
      "        value: 9.399999618530273\n",
      "        value: 42.70000076293945\n",
      "        value: 16.799999237060547\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "b'\\nc\\n5\\n\\x0efavorite_books\\x12#\\n!\\n\\rdeep learning\\n\\x10machine learning\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x1c\\n\\x04hour\\x12\\x14\\x12\\x12\\n\\x10\\x9a\\x99\\x99?ff\\x16A\\xcd\\xcc*Bff\\x86A'\n"
     ]
    }
   ],
   "source": [
    "example = tf.train.Example(features=features)\n",
    "print(example)\n",
    "\n",
    "serialized_example = example.SerializeToString()\n",
    "print(serialized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"tfrecord-basic\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "file_name = \"test.tfrecords\"\n",
    "filename_fullpath = os.path.join(output_dir, file_name)\n",
    "with tf.io.TFRecordWriter(filename_fullpath) as writter:\n",
    "    for i in range(3):\n",
    "        writter.write(serialized_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用tf.data api读取tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'\\nc\\n5\\n\\x0efavorite_books\\x12#\\n!\\n\\rdeep learning\\n\\x10machine learning\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x1c\\n\\x04hour\\x12\\x14\\x12\\x12\\n\\x10\\x9a\\x99\\x99?ff\\x16A\\xcd\\xcc*Bff\\x86A', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\nc\\n5\\n\\x0efavorite_books\\x12#\\n!\\n\\rdeep learning\\n\\x10machine learning\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x1c\\n\\x04hour\\x12\\x14\\x12\\x12\\n\\x10\\x9a\\x99\\x99?ff\\x16A\\xcd\\xcc*Bff\\x86A', shape=(), dtype=string)\n",
      "tf.Tensor(b'\\nc\\n5\\n\\x0efavorite_books\\x12#\\n!\\n\\rdeep learning\\n\\x10machine learning\\n\\x0c\\n\\x03age\\x12\\x05\\x1a\\x03\\n\\x01\\x10\\n\\x1c\\n\\x04hour\\x12\\x14\\x12\\x12\\n\\x10\\x9a\\x99\\x99?ff\\x16A\\xcd\\xcc*Bff\\x86A', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset([filename_fullpath])\n",
    "for serialize_example_tensor in dataset:\n",
    "    print(serialize_example_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从csv文件中生成tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: ['generate csv\\\\train_00.csv', 'generate csv\\\\train_01.csv', 'generate csv\\\\train_02.csv', 'generate csv\\\\train_03.csv', 'generate csv\\\\train_04.csv', 'generate csv\\\\train_05.csv', 'generate csv\\\\train_06.csv', 'generate csv\\\\train_07.csv', 'generate csv\\\\train_08.csv', 'generate csv\\\\train_09.csv', 'generate csv\\\\train_10.csv', 'generate csv\\\\train_11.csv', 'generate csv\\\\train_12.csv', 'generate csv\\\\train_13.csv', 'generate csv\\\\train_14.csv', 'generate csv\\\\train_15.csv', 'generate csv\\\\train_16.csv', 'generate csv\\\\train_17.csv', 'generate csv\\\\train_18.csv', 'generate csv\\\\train_19.csv']\n"
     ]
    }
   ],
   "source": [
    "source_dir = 'generate csv'\n",
    "def get_data_by_prefix(source_dir, prefix):\n",
    "    results = []\n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.startswith(prefix):\n",
    "            results.append(os.path.join(source_dir, file))\n",
    "    return results\n",
    "\n",
    "train_filenames = get_data_by_prefix(source_dir, prefix=\"train\")\n",
    "val_filenames = get_data_by_prefix(source_dir, prefix=\"val\")\n",
    "test_filenames = get_data_by_prefix(source_dir, prefix=\"test\")\n",
    "\n",
    "print(\"train:\", train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " def parse_csv_line(line, n_fields=9):\n",
    "        defs = [tf.constant(np.nan)] * n_fields\n",
    "        parse_line = tf.io.decode_csv(line, record_defaults=defs)\n",
    "        x = tf.stack(parse_line[:-1])\n",
    "        y = tf.stack(parse_line[-1:])\n",
    "        return x, y\n",
    "    \n",
    "def csv_reader_dataset(filenames, n_readers=5, batch_size=32, n_parse_thread=5,\n",
    "                      shuffler_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(1),\n",
    "                                cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffler_buffer_size)\n",
    "    dataset = dataset.map(parse_csv_line, num_parallel_calls=n_parse_thread)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_set = csv_reader_dataset(train_filenames)\n",
    "val_set = csv_reader_dataset(val_filenames)\n",
    "test_set = csv_reader_dataset(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 121\n"
     ]
    }
   ],
   "source": [
    "def serialize_example(x, y):\n",
    "    \"\"\"将从csv文件中读取的样本转化成tf.data.Example格式并且将其序列化\"\"\"\n",
    "    input_features = tf.train.FloatList(value=x)\n",
    "    label = tf.train.FloatList(value=y)\n",
    "    features = tf.train.Features(feature={\n",
    "        \"input_features\": tf.train.Feature(float_list=input_features),\n",
    "        \"label\": tf.train.Feature(float_list=label) \n",
    "    })\n",
    "    example = tf.train.Example(features=features)\n",
    "    return example.SerializeToString()\n",
    "\n",
    "def csv_dataset_to_tfrecord(base_filenames, dataset, n_shards, step_per_shard,\n",
    "                           compression_type=None):\n",
    "    options = tf.io.TFRecordOptions(compression_type=compression_type)\n",
    "    all_flienames = []\n",
    "    for shard in range(n_shards):\n",
    "        filename_fullpath = \"{}{:05d}-of-{:05d}\".format(base_filenames, shard, n_shards)\n",
    "        with tf.io.TFRecordWriter(os.path.join(filename_fullpath), options) as writter:\n",
    "            for x_batch, y_batch in dataset.take(step_per_shard):\n",
    "                for x_example, y_example in zip(x_batch, y_batch):\n",
    "                    writer.writer(serialize_example(x_example, y_example))\n",
    "        all_flienames.append(filename_fullpath)\n",
    "    return all_flienames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_shards = 20\n",
    "batch_size = 32\n",
    "train_step_per_shard = 11610 // batch_size // n_shards\n",
    "val_step_per_shard = 3880 // batch_size // n_shards\n",
    "test_step_per_shard = 5170 // batch_size // n_shards\n",
    "\n",
    "output_dir = \"generate_tfrecords\"\n",
    "if os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "train_basename = os.path.join(output_dir, \"train\")\n",
    "val_basename = os.path.join(output_dir, \"val\")\n",
    "test_basename = os.path.join(output_dir, \"test\")\n",
    "\n",
    "train_tgrecords_filenames = csv_dataset_to_tfrecord(base_filenames=train_basename,\n",
    "                                                    dataset=train_set,\n",
    "                                                    n_shards=n_shards,\n",
    "                                                    step_per_shard=train_step_per_shard,\n",
    "                                                    compression_type=None)\n",
    "val_tgrecords_filenames = csv_dataset_to_tfrecord(base_filenames=val_basename,\n",
    "                                                    dataset=val_set,\n",
    "                                                    n_shards=n_shards,\n",
    "                                                    step_per_shard=val_step_per_shard,\n",
    "                                                    compression_type=None)\n",
    "test_tgrecords_filenames = csv_dataset_to_tfrecord(base_filenames=test_filenames,\n",
    "                                                    dataset=test_set,\n",
    "                                                    n_shards=n_shards,\n",
    "                                                    step_per_shard=test_step_per_shard,\n",
    "                                                    compression_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 115\n"
     ]
    }
   ],
   "source": [
    "expret_features = {\n",
    "    \"input_features\": tf.io.FixedLenFeature([8], dtype=tf.float32),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.float32)\n",
    "}\n",
    "\n",
    "def parse_example(serialized_example):\n",
    "    example = tf.io.serialize_single_sparse(serialize_example, expret_features)\n",
    "    return example[\"input_features\"], example[\"label\"]\n",
    "\n",
    "def tfrecord_reader_dataset(filenames, n_readers=5, batch_size=32, n_parse_thread=5,\n",
    "                      shuffler_buffer_size=10000):\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filename: tf.data.TFRecordDataset(filename),\n",
    "                                cycle_length=n_readers)\n",
    "    dataset.shuffle(shuffler_buffer_size)\n",
    "    dataset = dataset.map(parse_exampl, num_parallel_calls=n_parse_thread)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-36ff52ad0dba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_reader_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_reader_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_reader_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 118\n"
     ]
    }
   ],
   "source": [
    "train_set = tfrecord-tfrecord_reader_dataset(train_tgrecords_filenames)\n",
    "val_set = tfrecord_reader_dataset(val_tgrecords_filenames)\n",
    "test_set = tfrecord_reader_dataset(test_tgrecords_filenames)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=[8]),\n",
    "    keras.layers.Dense(300, activation='relu'),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(\"adam\",loss='mse')\n",
    "history = model.fit(train_set,\n",
    "                epochs=10,\n",
    "                steps_per_epoch = 11160 // 32,\n",
    "                validation_steps = 3870 // 32,\n",
    "                validation_data=val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
